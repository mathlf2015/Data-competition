{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score,log_loss,f1_score\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from scipy import sparse\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA,NMF\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "df_train_operation = pd.read_csv(\"../data/operation_TRAIN_new.csv\")\n",
    "df_train_transaction = pd.read_csv(\"../data/transaction_TRAIN_new.csv\")\n",
    "df_label = pd.read_csv(\"../data/new_label.csv\")#trick修改后的label\n",
    "df_test_operation = pd.read_csv(\"../data/test_operation_round2.csv\")\n",
    "df_test_transaction = pd.read_csv(\"../data/test_transaction_round2.csv\")\n",
    "\n",
    "# 提交样例.csv\n",
    "sub = pd.read_csv(\"../data/new/submit_example.csv\")[[\"UID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def analyse(data,name,label='label'):\n",
    "    xx = data[[\"UID\",label]+name].drop_duplicates()\n",
    "    result = xx.groupby(name)[label].agg({'count':'count',\n",
    "                                              'sum':'sum'}).reset_index()\n",
    "    result['rate'] = result['sum']/(result['count']+20)\n",
    "    return result\n",
    "\n",
    "def tpr_weight_funtion(y_true,y_predict):\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 'TC_AUC',0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3,True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_feature(op,trans,label):\n",
    "    \n",
    "    op[\"ip1_sup_add_ip2_sub\"] = (op[\"ip1_sub\"] + op[\"ip2_sub\"]).astype(str)\n",
    "    op[\"os_version\"] = op[\"os\"].astype(str) + op[\"version\"].astype(str)\n",
    "\n",
    "    col = [i for i in op.columns[2:]if i not in [\"time\"] ]\n",
    "    for feature in col:\n",
    "        label = label.merge(op.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "        label =label.merge(op.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "        \n",
    "    label =label.merge(op.groupby(['UID'])[\"success\"].mean().reset_index(),on='UID',how='left')\n",
    "    \n",
    "    \n",
    "    col = [i for i in trans.columns[2:] if i not in [\"time\"] ]\n",
    "    for feature in col:\n",
    "        if trans_train[feature].dtype == 'object':\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "        else:\n",
    "            print(feature)\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].max().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].min().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].sum().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].mean().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].std().reset_index(),on='UID',how='left')\n",
    "            \n",
    "    op[\"null_sum\"] = op.isnull().sum(axis=1)\n",
    "    trans[\"null_sum\"] = trans.isnull().sum(axis=1)\n",
    "    \n",
    "    tmp = op.groupby(['UID'])['null_sum'].agg({'min_null_num': 'min',\n",
    "                                                                 'mean_null_num': 'mean',\n",
    "                                                                 'median_null_num': 'median',\n",
    "                                                                 'max_null_num': 'max',\n",
    "                                                                }).reset_index().fillna(0)\n",
    "    label = pd.merge(label,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = trans.groupby(['UID'])['null_sum'].agg({'min_null_num': 'min',\n",
    "                                                                 'mean_null_num': 'mean',\n",
    "                                                                 'median_null_num': 'median',\n",
    "                                                                 'max_null_num': 'max',\n",
    "                                                                }).reset_index().fillna(0)\n",
    "    label = pd.merge(label,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    t1 = trans[[\"UID\",\"trans_type2\"]]\n",
    "    t1[\"trans_type2_cnt\"] = 1\n",
    "    tmp = t1.groupby([\"UID\",\"trans_type2\"])[\"trans_type2_cnt\"].count().reset_index()\n",
    "    tmp = pd.pivot_table(tmp,index=\"UID\",columns=\"trans_type2\",values=\"trans_type2_cnt\").reset_index().fillna(0)\n",
    "    label = pd.merge(label,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    t1 = trans[[\"UID\", \"trans_type1\"]]\n",
    "    t1[\"trans_type1_cnt\"] = 1\n",
    "    tmp = t1.groupby([\"UID\", \"trans_type1\"])[\"trans_type1_cnt\"].count().reset_index()\n",
    "    tmp = pd.pivot_table(tmp, index=\"UID\", columns=\"trans_type1\", values=\"trans_type1_cnt\").reset_index().fillna(0)\n",
    "    label = pd.merge(label, tmp, on=\"UID\", how=\"left\")\n",
    "    \n",
    "     \n",
    "    print(\"df shape\",label.shape)\n",
    "    \n",
    "    return label\n",
    "\n",
    "def get_magic_fea(df_train,df_test,label,sub,col):\n",
    "    for i in col:\n",
    "        print(i)\n",
    "        df_train[i] = df_train[i].fillna(\"missing\").astype(str)\n",
    "        df_test[i] = df_test[i].fillna(\"missing\").astype(str)\n",
    "        common = [x for x in list(set(df_train[i].tolist()).intersection(set(df_test[i].tolist()))) if x not in [\"missing\"]]\n",
    "        df_train[\"magic_\"+i] = df_train[i].isin(common).astype(int)\n",
    "        df_test[\"magic_\"+i] = df_test[i].isin(common).astype(int)\n",
    "        label =label.merge(df_train.groupby(['UID'])[\"magic_\"+i].mean().reset_index(),on='UID',how='left')\n",
    "        sub =sub.merge(df_test.groupby(['UID'])[\"magic_\"+i].mean().reset_index(),on='UID',how='left')\n",
    "    return label,sub\n",
    "    \n",
    "def gen_leak_2(df):\n",
    "    res = df[[\"UID\"]].drop_duplicates()\n",
    "    \n",
    "    tmp = df.groupby([\"ip1\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"ip1\",\"ip1_nunique\"]\n",
    "    tmp.sort_values(by=\"ip1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"ip1\"]].drop_duplicates(),tmp,on=\"ip1\",how=\"left\")[[\"UID\",\"ip1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['ip1_nunique'].agg({'max_ip1_nunique_num_2': 'max','std_ip1_nunique_num_2': 'std'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\") \n",
    "     \n",
    "    tmp = df.groupby([\"ip1_sub\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"ip1_sub\",\"ip1_sub_nunique\"]\n",
    "    tmp.sort_values(by=\"ip1_sub_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"ip1_sub\"]].drop_duplicates(),tmp,on=\"ip1_sub\",how=\"left\")[[\"UID\",\"ip1_sub_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['ip1_sub_nunique'].agg({'max_ip1_sub_nunique_num_2': 'max','std_ip2_nunique_num_2': 'std'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"device_code1\"])[\"merchant\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device_code1\",\"device_code1_nunique\"]\n",
    "    tmp.sort_values(by=\"device_code1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device_code1\"]].drop_duplicates(),tmp,on=\"device_code1\",how=\"left\")[[\"UID\",\"device_code1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device_code1_nunique'].agg({'max_device_code1_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"device_code1\"])[\"ip1\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device_code1\",\"device_code1_nunique\"]\n",
    "    tmp.sort_values(by=\"device_code1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device_code1\"]].drop_duplicates(),tmp,on=\"device_code1\",how=\"left\")[[\"UID\",\"device_code1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device_code1_nunique'].agg({'max_device_code1_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"device_code1\"])[\"geo_code\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device_code1\",\"device_code1_nunique\"]\n",
    "    tmp.sort_values(by=\"device_code1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device_code1\"]].drop_duplicates(),tmp,on=\"device_code1\",how=\"left\")[[\"UID\",\"device_code1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device_code1_nunique'].agg({'max_device_code1_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "      \n",
    "    tmp = df.groupby([\"device_code1\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device_code1\",\"device_code1_nunique\"]\n",
    "    tmp.sort_values(by=\"device_code1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device_code1\"]].drop_duplicates(),tmp,on=\"device_code1\",how=\"left\")[[\"UID\",\"device_code1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device_code1_nunique'].agg({'max_device_code1_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"device_code2\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device_code2\",\"device_code2_nunique\"]\n",
    "    tmp.sort_values(by=\"device_code2_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device_code2\"]].drop_duplicates(),tmp,on=\"device_code2\",how=\"left\")[[\"UID\",\"device_code2_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device_code2_nunique'].agg({'max_device_code2_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"device_code3\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device_code3\",\"device_code3_nunique\"]\n",
    "    tmp.sort_values(by=\"device_code3_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device_code3\"]].drop_duplicates(),tmp,on=\"device_code3\",how=\"left\")[[\"UID\",\"device_code3_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device_code3_nunique'].agg({'max_device_code3_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"acc_id1\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"acc_id1\",\"acc_id1_nunique\"]\n",
    "    tmp.sort_values(by=\"acc_id1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"acc_id1\"]].drop_duplicates(),tmp,on=\"acc_id1\",how=\"left\")[[\"UID\",\"acc_id1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['acc_id1_nunique'].agg({'max_acc_id1_nunique_num_2': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\") \n",
    "    \n",
    "    tmp = df.groupby([\"device1\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device1\",\"device1_nunique\"]\n",
    "    tmp.sort_values(by=\"device1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device1\"]].drop_duplicates(),tmp,on=\"device1\",how=\"left\")[[\"UID\",\"device1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device1_nunique'].agg({'max_device1_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"mac1\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"mac1\",\"mac1_nunique\"]\n",
    "    tmp.sort_values(by=\"mac1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"mac1\"]].drop_duplicates(),tmp,on=\"mac1\",how=\"left\")[[\"UID\",\"mac1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['mac1_nunique'].agg({'max_mac1_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "     \n",
    "    return res\n",
    "    \n",
    "def gen_leak_1(df):\n",
    "    res = df[[\"UID\"]].drop_duplicates()\n",
    "    \n",
    "    tmp = df.groupby([\"ip1\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"ip1\",\"ip1_nunique\"]\n",
    "    tmp.sort_values(by=\"ip1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"ip1\"]].drop_duplicates(),tmp,on=\"ip1\",how=\"left\")[[\"UID\",\"ip1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['ip1_nunique'].agg({'max_ip1_nunique_num': 'max','std_ip1_nunique_num': 'std'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\") \n",
    "    \n",
    "    tmp = df.groupby([\"ip2\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"ip2\",\"ip2_nunique\"]\n",
    "    tmp.sort_values(by=\"ip2_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"ip2\"]].drop_duplicates(),tmp,on=\"ip2\",how=\"left\")[[\"UID\",\"ip2_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['ip2_nunique'].agg({'max_ip2_nunique_num': 'max','std_ip2_nunique_num': 'std'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"ip1\",\"ip2\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"ip1\",\"ip2\",\"ip1_ip2_nunique\"]\n",
    "    tmp.sort_values(by=\"ip1_ip2_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"ip1\",\"ip2\"]].drop_duplicates(),tmp,on=\"ip2\",how=\"left\")[[\"UID\",\"ip1_ip2_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['ip1_ip2_nunique'].agg({'max_ip1_ip2_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    tmp = df.groupby([\"ip1_sub\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"ip1_sub\",\"ip1_sub_nunique\"]\n",
    "    tmp.sort_values(by=\"ip1_sub_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"ip1_sub\"]].drop_duplicates(),tmp,on=\"ip1_sub\",how=\"left\")[[\"UID\",\"ip1_sub_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['ip1_sub_nunique'].agg({'max_ip1_sub_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"ip2_sub\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"ip2_sub\",\"ip2_sub_nunique\"]\n",
    "    tmp.sort_values(by=\"ip2_sub_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"ip2_sub\"]].drop_duplicates(),tmp,on=\"ip2_sub\",how=\"left\")[[\"UID\",\"ip2_sub_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['ip2_sub_nunique'].agg({'max_ip2_sub_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"device_code1\"])[\"geo_code\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device_code1\",\"device_code1_nunique\"]\n",
    "    tmp.sort_values(by=\"device_code1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device_code1\"]].drop_duplicates(),tmp,on=\"device_code1\",how=\"left\")[[\"UID\",\"device_code1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device_code1_nunique'].agg({'max_device_code1_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"device_code1\"])[\"ip1\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device_code1\",\"device_code1_nunique\"]\n",
    "    tmp.sort_values(by=\"device_code1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device_code1\"]].drop_duplicates(),tmp,on=\"device_code1\",how=\"left\")[[\"UID\",\"device_code1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device_code1_nunique'].agg({'max_device_code1_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "\n",
    "    tmp = df.groupby([\"device_code1\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device_code1\",\"device_code1_nunique\"]\n",
    "    tmp.sort_values(by=\"device_code1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device_code1\"]].drop_duplicates(),tmp,on=\"device_code1\",how=\"left\")[[\"UID\",\"device_code1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device_code1_nunique'].agg({'max_device_code1_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"device_code2\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device_code2\",\"device_code2_nunique\"]\n",
    "    tmp.sort_values(by=\"device_code2_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device_code2\"]].drop_duplicates(),tmp,on=\"device_code2\",how=\"left\")[[\"UID\",\"device_code2_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device_code2_nunique'].agg({'max_device_code2_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"device_code3\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device_code3\",\"device_code3_nunique\"]\n",
    "    tmp.sort_values(by=\"device_code3_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device_code3\"]].drop_duplicates(),tmp,on=\"device_code3\",how=\"left\")[[\"UID\",\"device_code3_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device_code3_nunique'].agg({'max_device_code3_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby([\"device1\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"device1\",\"device1_nunique\"]\n",
    "    tmp.sort_values(by=\"device1_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"device1\"]].drop_duplicates(),tmp,on=\"device1\",how=\"left\")[[\"UID\",\"device1_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['device1_nunique'].agg({'max_device1_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    \n",
    "    tmp = df.groupby([\"mac2\"])[\"UID\"].nunique().reset_index()\n",
    "    tmp.columns =[\"mac2\",\"mac2_nunique\"]\n",
    "    tmp.sort_values(by=\"mac2_nunique\",ascending=False)\n",
    "    tmp = pd.merge(df[[\"UID\",\"mac2\"]].drop_duplicates(),tmp,on=\"mac2\",how=\"left\")[[\"UID\",\"mac2_nunique\"]]\n",
    "    tmp = tmp.groupby(['UID'])['mac2_nunique'].agg({'max_mac2_nunique_num': 'max'}).reset_index().fillna(0)\n",
    "    res = pd.merge(res,tmp,on=\"UID\",how=\"left\")\n",
    "    \n",
    "    print(res.head(5))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def feature_encoding(df_label,op_train,op_test,train,test,col):\n",
    "    op_train = pd.merge(op_train,df_label,on=\"UID\",how=\"left\")\n",
    "    for feature in col:\n",
    "        print(op_train.columns)\n",
    "        tmp = analyse(op_train,[feature],label='Tag')\n",
    "        tmp = tmp.sort_values(by=feature,ascending=False)\n",
    "        print(tmp.head(10))\n",
    "        xx_train = pd.merge(op_train,tmp,on=feature,how=\"left\")\n",
    "        xx_test = pd.merge(op_test,tmp,on=feature,how=\"left\")\n",
    "        print(xx_train.columns)\n",
    "        tmp = xx_train.groupby(['UID'])['rate'].agg({\n",
    "                                                                 'median_rate': 'median',\n",
    "                                                                 'max_rate': 'max',\n",
    "                                                                }).reset_index().fillna(0)\n",
    "        train = pd.merge(train,tmp,on=\"UID\",how=\"left\")\n",
    "        \n",
    "        tmp = xx_test.groupby(['UID'])['rate'].agg({\n",
    "                                                                 'median_rate': 'median',\n",
    "                                                                 'max_rate': 'max',\n",
    "                                                                }).reset_index().fillna(0)\n",
    "        test = pd.merge(test,tmp,on=\"UID\",how=\"left\")\n",
    "    del op_train[\"Tag\"]\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "    return train,test\n",
    "\n",
    "\n",
    "\n",
    "def get_w2v_fea(op_train, op_test, train, test,i):\n",
    "    df = pd.read_csv(\"../data/new/w2v/merchantnew.csv\")\n",
    "    df = df.drop_duplicates([i])\n",
    "    op_train = op_train[[\"UID\", i]]\n",
    "    op_test = op_test[[\"UID\", i]]\n",
    "    op_train = pd.merge(op_train, df, on=i, how='left')\n",
    "    op_test = pd.merge(op_test, df, on=i, how='left')\n",
    "    col = [x for x in op_train.columns if \"%sW\"%i in x]\n",
    "    for feature in col:\n",
    "        tmp = op_train.groupby(['UID'])[feature].agg({\n",
    "            'median_rate': 'mean',\n",
    "        }).reset_index().fillna(0)\n",
    "        train = pd.merge(train, tmp, on=\"UID\", how=\"left\")\n",
    "\n",
    "        tmp = op_test.groupby(['UID'])[feature].agg({\n",
    "            'median_rate': 'mean',\n",
    "        }).reset_index().fillna(0)\n",
    "        test = pd.merge(test, tmp, on=\"UID\", how=\"left\")\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def get_w2v_fea3(op_train, op_test, train, test,i):\n",
    "    df = pd.read_csv(\"../data/new/w2v/xxnew_.csv\")\n",
    "    df = df.drop_duplicates([i])\n",
    "    op_train[\"xx\"] = op_train[\"day\"].astype(str) + op_train[\"time\"].apply(lambda x:\"\".join(x.split(\":\")[:2]))\n",
    "    op_test[\"xx\"] = op_test[\"day\"].astype(str) + op_test[\"time\"].apply(lambda x:\"\".join(x.split(\":\")[:2]))\n",
    "    op_train[\"xx\"] = op_train[\"xx\"].astype(int)\n",
    "    op_test[\"xx\"] = op_test[\"xx\"].astype(int)\n",
    "    op_train = op_train[[\"UID\", i]]\n",
    "    op_test = op_test[[\"UID\", i]]\n",
    "    \n",
    "    print(op_train.dtypes)\n",
    "    print(df.dtypes)\n",
    "    op_train = pd.merge(op_train, df, on=i, how='left')\n",
    "    op_test = pd.merge(op_test, df, on=i, how='left')\n",
    "    col = [x for x in op_train.columns if \"%sW\"%i in x]\n",
    "    for feature in col:\n",
    "        tmp = op_train.groupby(['UID'])[feature].agg({\n",
    "            'median_rate': 'mean',\n",
    "        }).reset_index().fillna(0)\n",
    "        train = pd.merge(train, tmp, on=\"UID\", how=\"left\")\n",
    "\n",
    "        tmp = op_test.groupby(['UID'])[feature].agg({\n",
    "            'median_rate': 'mean',\n",
    "        }).reset_index().fillna(0)\n",
    "        test = pd.merge(test, tmp, on=\"UID\", how=\"left\")\n",
    "\n",
    "    return train, test\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day\n",
      "trans_amt\n",
      "bal\n",
      "trans_type2\n",
      "market_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:128: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:135: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape (31179, 141)\n",
      "day\n",
      "trans_amt\n",
      "bal\n",
      "trans_type2\n",
      "market_type\n",
      "df shape (31588, 138)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:318: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:326: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:333: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:342: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:349: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:364: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:371: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:379: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:386: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:393: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:400: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:414: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     UID  max_ip1_nunique_num  std_ip1_nunique_num  max_ip2_nunique_num  \\\n",
      "0  10035                 32.0            13.954471                  0.0   \n",
      "1  16264                  1.0             0.000000                  1.0   \n",
      "2  13162                  2.0             0.447214                  0.0   \n",
      "3  21392                  2.0             0.333333                  0.0   \n",
      "4  18599                220.0            27.270864                  0.0   \n",
      "\n",
      "   std_ip2_nunique_num  max_ip1_ip2_nunique_num  max_ip1_sub_nunique_num  \\\n",
      "0                  0.0                      0.0                    134.0   \n",
      "1                  0.0                      0.0                     16.0   \n",
      "2                  0.0                      0.0                     47.0   \n",
      "3                  0.0                      0.0                     32.0   \n",
      "4                  0.0                      0.0                    220.0   \n",
      "\n",
      "   max_ip2_sub_nunique_num  max_device_code1_nunique_num_x  \\\n",
      "0                      0.0                             5.0   \n",
      "1                      4.0                             2.0   \n",
      "2                      0.0                             1.0   \n",
      "3                      0.0                             1.0   \n",
      "4                      0.0                             0.0   \n",
      "\n",
      "   max_device_code1_nunique_num_y  max_device_code1_nunique_num  \\\n",
      "0                            12.0                           1.0   \n",
      "1                            15.0                           1.0   \n",
      "2                             4.0                           1.0   \n",
      "3                             9.0                           1.0   \n",
      "4                             5.0                           1.0   \n",
      "\n",
      "   max_device_code2_nunique_num  max_device_code3_nunique_num  \\\n",
      "0                           1.0                           0.0   \n",
      "1                           1.0                         799.0   \n",
      "2                           1.0                         799.0   \n",
      "3                           1.0                           0.0   \n",
      "4                           1.0                           0.0   \n",
      "\n",
      "   max_device1_nunique_num  max_mac2_nunique_num  \n",
      "0                   6443.0                9716.0  \n",
      "1                   6184.0                9716.0  \n",
      "2                   2690.0                9716.0  \n",
      "3                   2690.0                9716.0  \n",
      "4                   6184.0                   1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:218: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:227: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:234: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:241: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:248: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:256: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:263: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:270: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:277: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:284: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:291: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      UID  max_ip1_nunique_num  std_ip1_nunique_num  max_ip2_nunique_num  \\\n",
      "0  119982                  2.0             0.500000                  0.0   \n",
      "1  129994                  1.0             0.000000                  0.0   \n",
      "2  112135                 94.0            44.477522                  0.0   \n",
      "3  121943                  1.0             0.000000                  0.0   \n",
      "4  114176                  1.0             0.000000                  0.0   \n",
      "\n",
      "   std_ip2_nunique_num  max_ip1_ip2_nunique_num  max_ip1_sub_nunique_num  \\\n",
      "0                  0.0                      0.0                      9.0   \n",
      "1                  0.0                      0.0                     14.0   \n",
      "2                  0.0                      0.0                    136.0   \n",
      "3                  0.0                      0.0                      7.0   \n",
      "4                  0.0                      0.0                     24.0   \n",
      "\n",
      "   max_ip2_sub_nunique_num  max_device_code1_nunique_num_x  \\\n",
      "0                      0.0                             1.0   \n",
      "1                      0.0                             1.0   \n",
      "2                      0.0                             1.0   \n",
      "3                      0.0                             1.0   \n",
      "4                      0.0                             5.0   \n",
      "\n",
      "   max_device_code1_nunique_num_y  max_device_code1_nunique_num  \\\n",
      "0                             4.0                           1.0   \n",
      "1                             5.0                           1.0   \n",
      "2                             3.0                           1.0   \n",
      "3                             5.0                           1.0   \n",
      "4                            32.0                           1.0   \n",
      "\n",
      "   max_device_code2_nunique_num  max_device_code3_nunique_num  \\\n",
      "0                           1.0                       16134.0   \n",
      "1                           1.0                       16134.0   \n",
      "2                           1.0                       16134.0   \n",
      "3                           1.0                           0.0   \n",
      "4                           1.0                       16134.0   \n",
      "\n",
      "   max_device1_nunique_num  max_mac2_nunique_num  \n",
      "0                   7197.0                   1.0  \n",
      "1                   5452.0                   1.0  \n",
      "2                   2622.0                   1.0  \n",
      "3                   2622.0               13379.0  \n",
      "4                   6050.0                   1.0  \n",
      "(31179, 168)\n",
      "(31588, 165)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "op_train = df_train_operation\n",
    "op_test =df_test_operation\n",
    "trans_train = df_train_transaction\n",
    "trans_test = df_test_transaction\n",
    "y = df_label\n",
    "\n",
    "\n",
    "train = get_feature(op_train,trans_train,y).fillna(-1)\n",
    "test = get_feature(op_test,trans_test,sub).fillna(-1)\n",
    "\n",
    "df_train_leak_1 = gen_leak_1(df_train_operation)\n",
    "df_train_leak_2 = gen_leak_2(df_train_transaction)\n",
    "train = pd.merge(train,df_train_leak_1,on=\"UID\",how=\"left\")\n",
    "train = pd.merge(train,df_train_leak_2,on=\"UID\",how=\"left\")\n",
    "\n",
    "df_test_leak_1 = gen_leak_1(df_test_operation)\n",
    "df_test_leak_2 = gen_leak_2(df_test_transaction)\n",
    "test = pd.merge(test,df_test_leak_1,on=\"UID\",how=\"left\")\n",
    "test = pd.merge(test,df_test_leak_2,on=\"UID\",how=\"left\")\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train = train.drop(['Tag'],axis = 1).fillna(-1)\n",
    "label = y['Tag']\n",
    "\n",
    "test_id = test['UID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31179, 167) (31588, 165)\n",
      "Index(['UID', 'day', 'mode', 'success', 'time', 'os', 'version', 'device1',\n",
      "       'device2', 'device_code1', 'device_code2', 'device_code3', 'mac1',\n",
      "       'mac2', 'ip1', 'ip2', 'wifi', 'geo_code', 'ip1_sub', 'ip2_sub',\n",
      "       'ip1_sup_add_ip2_sub', 'os_version', 'null_sum', 'geo_2', 'device2_2',\n",
      "       'Tag'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    os  count   sum      rate\n",
      "6  200  16700  2857  0.170873\n",
      "5  107     83     2  0.019417\n",
      "4  105      4     4  0.166667\n",
      "3  104    179    25  0.125628\n",
      "2  103   6297  2188  0.346367\n",
      "1  102  24054  2769  0.115020\n",
      "0  101   4211  1077  0.254550\n",
      "Index(['UID', 'day', 'mode', 'success', 'time', 'os', 'version', 'device1',\n",
      "       'device2', 'device_code1', 'device_code2', 'device_code3', 'mac1',\n",
      "       'mac2', 'ip1', 'ip2', 'wifi', 'geo_code', 'ip1_sub', 'ip2_sub',\n",
      "       'ip1_sup_add_ip2_sub', 'os_version', 'null_sum', 'geo_2', 'device2_2',\n",
      "       'Tag', 'count', 'sum', 'rate'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:439: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:445: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['UID', 'day', 'mode', 'success', 'time', 'os', 'version', 'device1',\n",
      "       'device2', 'device_code1', 'device_code2', 'device_code3', 'mac1',\n",
      "       'mac2', 'ip1', 'ip2', 'wifi', 'geo_code', 'ip1_sub', 'ip2_sub',\n",
      "       'ip1_sup_add_ip2_sub', 'os_version', 'null_sum', 'geo_2', 'device2_2',\n",
      "       'Tag'],\n",
      "      dtype='object')\n",
      "   version  count   sum      rate\n",
      "37   7.1.0      3     0  0.000000\n",
      "36   7.0.9  17432  1637  0.093800\n",
      "35   7.0.8      1     0  0.000000\n",
      "34   7.0.7   4004   927  0.230368\n",
      "33   7.0.6      2     0  0.000000\n",
      "32   7.0.5  16753  2030  0.121028\n",
      "31   7.0.2   3463   357  0.102498\n",
      "30   7.0.1     62     1  0.012195\n",
      "29   7.0.0   1020   712  0.684615\n",
      "28   6.6.3    731   513  0.683089\n"
     ]
    }
   ],
   "source": [
    "print(train.shape,test.shape)\n",
    "trans_train[\"merchant_2\"] = trans_train[\"merchant\"].fillna(\"missing\").apply(lambda x:x[:2])\n",
    "trans_test[\"merchant_2\"] = trans_test[\"merchant\"].fillna(\"missing\").apply(lambda x:x[:2])\n",
    "\n",
    "op_train[\"geo_2\"] = op_train[\"geo_code\"].fillna(\"missing\").apply(lambda x:x[:2])\n",
    "op_test[\"geo_2\"] = op_test[\"geo_code\"].fillna(\"missing\").apply(lambda x:x[:2])\n",
    "trans_train[\"geo_2\"] = trans_train[\"geo_code\"].fillna(\"missing\").apply(lambda x:x[:2])\n",
    "trans_test[\"geo_2\"] = trans_test[\"geo_code\"].fillna(\"missing\").apply(lambda x:x[:2])\n",
    "\n",
    "train,test = feature_encoding(df_label,op_train,op_test,train,test,[\"os\",\"version\",\"geo_2\"])\n",
    "train,test = feature_encoding(df_label,trans_train,trans_test,train,test,[\"channel\",\"merchant_2\",\"geo_2\"])\n",
    "print(train.shape,test.shape)\n",
    "# (31179, 178) (31198, 179)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train,test = get_w2v_fea(trans_train,trans_test,train,test,\"merchant\")\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i in test.columns if i  not in train.columns])\n",
    "print([i for i in train.columns if i  not in test.columns])\n",
    "\n",
    "train = train.drop(['UID','Tag',103.0, '3f469aa3836e71cb', '4adc3de71fe1a83c', '9d7dd7b80e806024', 'd9c417304a5ae70c'],axis = 1).fillna(-1)\n",
    "test = test.drop(['UID','0c23319fa2efdd9f', '768160899ae359f6', 'd0c5bc6f28f5ad35'],axis = 1).fillna(-1)\n",
    "print(train.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=100, reg_alpha=3, reg_lambda=5, max_depth=6,\n",
    "#     n_estimators=5000, objective=\"binary\", subsample=0.9, colsample_bytree=0.77, subsample_freq=1, learning_rate=0.05,\n",
    "#     random_state=1000, n_jobs=16, min_child_weight=4, min_child_samples=5, min_split_gain=0,class_weight={0:1,1:3})\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=100, reg_alpha=3, reg_lambda=5, max_depth=-1,\n",
    "    n_estimators=5000, objective=\"binary\", subsample=0.9, colsample_bytree=0.77, subsample_freq=1, learning_rate=0.05,\n",
    "    random_state=1000, n_jobs=16, min_child_weight=4, min_child_samples=5, min_split_gain=0)\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "\n",
    "best_score = []\n",
    "\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test_id.shape[0])\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train, label)):\n",
    "    lgb_model.fit(train.iloc[train_index], label.iloc[train_index], verbose=50,\n",
    "                  eval_set=[(train.iloc[train_index], label.iloc[train_index]),\n",
    "                            (train.iloc[test_index], label.iloc[test_index])], early_stopping_rounds=30)\n",
    "    best_score.append(lgb_model.best_score_['valid_1']['binary_logloss'])\n",
    "    print(best_score)\n",
    "    oof_preds[test_index] = lgb_model.predict_proba(train.iloc[test_index], num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "\n",
    "    test_pred = lgb_model.predict_proba(test, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    sub_preds += test_pred / 5\n",
    "    #print('test mean:', test_pred.mean())\n",
    "    #predict_result['predicted_score'] = predict_result['predicted_score'] + test_pred\n",
    "    \n",
    "m = tpr_weight_funtion(y_predict=oof_preds,y_true=label)\n",
    "print(m[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8417836498761354\n"
     ]
    }
   ],
   "source": [
    "m = tpr_weight_funtion(y_predict=oof_preds,y_true=label)\n",
    "print(m[1])\n",
    "sub['Tag'] = sub_preds\n",
    "sub.to_csv('../submit/baseline_%s.csv'%str(m[1]),index=False)\n",
    "# 0.8139556592765461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
